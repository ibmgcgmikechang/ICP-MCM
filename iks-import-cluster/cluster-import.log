secret/klusterlet-bootstrap configured
serviceaccount/klusterlet-sa created
clusterrolebinding.rbac.authorization.k8s.io/klusterlet-crb created
secret/klusterlet-config created
job.batch/install-klusterlet created
Using /installer/ansible.cfg as config file

PLAY [Installing Klusterlet] ***************************************************

TASK [kubectl-config : Checking if kube-config exist or not] *******************
changed: [127.0.0.1 -> localhost] => changed=true 
  cmd: |-
    if kubectl get service &>/dev/null; then
     echo "in-cluster.yaml"
     elif ls -H /var/ansible/kubeconfig &>/dev/null; then
     echo "use-exist.yaml"
     else
     echo "create-one.yaml"
     fi
  delta: '0:00:00.960504'
  end: '2019-07-04 02:29:03.707859'
  rc: 0
  start: '2019-07-04 02:29:02.747355'
  stderr: ''
  stderr_lines: <omitted>
  stdout: in-cluster.yaml
  stdout_lines: <omitted>

TASK [kubectl-config : include_tasks] ******************************************

TASK [klusterlet-variables : Set install_klusterlet_playbook] ******************
ok: [127.0.0.1] => changed=false 
  ansible_facts:
    cluster_name: ''
    install_klusterlet_playbook: true

TASK [klusterlet-variables : Enable management services] ***********************
ok: [127.0.0.1] => changed=false 
  ansible_facts:
    management_services:
      cert-manager: enabled
      monitoring: enabled
      multicluster-endpoint: enabled
      tiller: enabled

TASK [klusterlet-variables : Setting variable for tiller] **********************
ok: [127.0.0.1] => changed=false 
  ansible_facts:
    tiller_host: localhost
    tiller_node_selector: {}

TASK [klusterlet-variables : Setting variable for cert-manager] ****************
ok: [127.0.0.1] => changed=false 
  ansible_facts:
    cert_manager_node_selector: {}

TASK [klusterlet-variables : Setting variable for monitoring] ******************
ok: [127.0.0.1] => changed=false 
  ansible_facts:
    monitoring:
      alertmanager:
        enabled: false
      clusterAddress: 127.0.0.1
      clusterDomain: cluster.local
      clusterName: mycluster
      clusterPort: 8443
      collectdExporter:
        enabled: false
      crdCreation: false
      elasticsearchExporter:
        enabled: false
      grafana:
        enabled: false
      kubeStateMetrics:
        enabled: false
      mode: standard
      nodeExporter:
        enabled: false
      prometheus:
        etcdTarget:
          enabled: false
      router:
        subjectAlt: 127.0.0.1

TASK [klusterlet-variables : Get Kubernetes server version] ********************
changed: [127.0.0.1] => changed=true 
  cmd: kubectl version --short | grep Server
  delta: '0:00:00.509173'
  end: '2019-07-04 02:29:05.432280'
  rc: 0
  start: '2019-07-04 02:29:04.923107'
  stderr: ''
  stderr_lines: <omitted>
  stdout: 'Server Version: v1.13.7+IKS'
  stdout_lines: <omitted>

TASK [klusterlet-variables : Find ibmcloud-cluster-info configmap] *************
changed: [127.0.0.1] => changed=true 
  cmd: kubectl get configmap -n kube-public | grep ibmcloud-cluster-info | wc -l
  delta: '0:00:00.751752'
  end: '2019-07-04 02:29:06.469777'
  rc: 0
  start: '2019-07-04 02:29:05.718025'
  stderr: No resources found.
  stderr_lines: <omitted>
  stdout: '0'
  stdout_lines: <omitted>

TASK [klusterlet-variables : Find mcm.ibm.com API] *****************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [klusterlet-variables : Get proxy ingress HTTPS port] *********************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [klusterlet-variables : Find project.openshift.io API] ********************
changed: [127.0.0.1] => changed=true 
  cmd: kubectl api-versions | grep '^project.openshift.io/*' | wc -l
  delta: '0:00:00.661684'
  end: '2019-07-04 02:29:07.641850'
  rc: 0
  start: '2019-07-04 02:29:06.980166'
  stderr: ''
  stderr_lines: <omitted>
  stdout: '0'
  stdout_lines: <omitted>

TASK [klusterlet-variables : Find omsagent service account] ********************
changed: [127.0.0.1] => changed=true 
  cmd: kubectl get sa -n kube-system | grep omsagent | wc -l
  delta: '0:00:00.559358'
  end: '2019-07-04 02:29:08.492158'
  rc: 0
  start: '2019-07-04 02:29:07.932800'
  stderr: ''
  stderr_lines: <omitted>
  stdout: '0'
  stdout_lines: <omitted>

TASK [klusterlet-variables : Set deployed_on_hub default] **********************
ok: [127.0.0.1] => changed=false 
  ansible_facts:
    deployed_on_hub: false

TASK [klusterlet-variables : Set deployed_on_hub] ******************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [klusterlet-variables : Set variables for ICP] ****************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [klusterlet-variables : Set variables for ICP on OpenShift] ***************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [klusterlet-variables : Set variables for ICP on IKS] *********************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [klusterlet-variables : Set variables for OpenShift] **********************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [klusterlet-variables : Set variables for IKS] ****************************
ok: [127.0.0.1] => changed=false 
  ansible_facts:
    kubernetes_cluster_type: other
    multicluster_endpoint:
      clusterLabels:
        cloud: IBM
        vendor: IKS
      klusterlet:
        ingressType: None
        serviceType: LoadBalancer

TASK [klusterlet-variables : Set variables for EKS] ****************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [klusterlet-variables : Set variables for GKE] ****************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [klusterlet-variables : Set variables for AKS] ****************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [klusterlet-variables : Validate clusterName] *****************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [klusterlet-variables : Validate clusterNamespace] ************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [klusterlet-variables : Check clusterName and clusterNamespace when not deployed_on_hub] ***
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [klusterlet-variables : Check clusterName and clusterNamespace when deployed_on_hub] ***
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [openshift-scc : SCC for icp services on openshift] ***********************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [rbac-config : Creating rbac roles] ***************************************
changed: [127.0.0.1 -> localhost] => changed=true 
  cmd: |-
    index=0
     retries=100
     while true; do
     sleep 6
     index=$(( index + 1 ))
     if [[ $index -eq $retries ]]; then
     echo "Failed to create rbac roles"
     exit 1
     fi
     kubectl api-versions | grep -w rbac.authorization.k8s.io/v1 && kubectl apply --force --overwrite=true -f /installer/playbook/roles/rbac-config/files/rbac.yaml && break || continue
     done
  delta: '0:00:07.326283'
  end: '2019-07-04 02:29:16.997721'
  rc: 0
  start: '2019-07-04 02:29:09.671438'
  stderr: ''
  stderr_lines: <omitted>
  stdout: |-
    rbac.authorization.k8s.io/v1
    clusterrole.rbac.authorization.k8s.io/icp:edit configured
    clusterrole.rbac.authorization.k8s.io/icp-edit-aggregate unchanged
    clusterrole.rbac.authorization.k8s.io/icp:operate configured
    clusterrole.rbac.authorization.k8s.io/icp-operate-aggregate unchanged
    clusterrole.rbac.authorization.k8s.io/icp:view configured
    clusterrole.rbac.authorization.k8s.io/icp-view-aggregate unchanged
    clusterrole.rbac.authorization.k8s.io/icp:admin configured
    clusterrole.rbac.authorization.k8s.io/icp-admin-aggregate unchanged
    clusterrole.rbac.authorization.k8s.io/extension unchanged
    clusterrolebinding.rbac.authorization.k8s.io/admin-users unchanged
    clusterrole.rbac.authorization.k8s.io/ibm-restricted-clusterrole unchanged
    clusterrole.rbac.authorization.k8s.io/ibm-anyuid-clusterrole unchanged
    clusterrole.rbac.authorization.k8s.io/ibm-anyuid-hostpath-clusterrole unchanged
    clusterrole.rbac.authorization.k8s.io/ibm-anyuid-hostaccess-clusterrole unchanged
    clusterrole.rbac.authorization.k8s.io/ibm-privileged-clusterrole unchanged
    clusterrolebinding.rbac.authorization.k8s.io/ibm-privileged-psp-users unchanged
    clusterrolebinding.rbac.authorization.k8s.io/system:node unchanged
    clusterrole.rbac.authorization.k8s.io/icp:teamadmin unchanged
    clusterrolebinding.rbac.authorization.k8s.io/system:node-bootstrapper unchanged
    clusterrolebinding.rbac.authorization.k8s.io/system:certificates.k8s.io:certificatesigningrequests:nodeclient unchanged
    clusterrolebinding.rbac.authorization.k8s.io/system:certificates.k8s.io:certificatesigningrequests:selfnodeclient unchanged
    clusterrolebinding.rbac.authorization.k8s.io/system:certificates.k8s.io:certificatesigningrequests:selfnodeserver unchanged
  stdout_lines: <omitted>

TASK [root-ca-certs : Ensuring that the certificate directory exist] ***********
changed: [127.0.0.1] => changed=true 
  gid: 0
  group: root
  mode: '0700'
  owner: root
  path: /var/ansible/cfc-certs/root-ca
  size: 4096
  state: directory
  uid: 0

TASK [root-ca-certs : Generating Root CA certificates] *************************
changed: [127.0.0.1] => changed=true 
  cmd: openssl req -newkey rsa:4096 -sha256 -nodes -keyout ca.key -x509 -days 3650 -out ca.crt -subj "/C=US/ST=New York/L=Armonk/O=IBM Cloud Private/CN=www.ibm.com" -config /installer/playbook/roles/root-ca-certs/files/ca-cert.cnf
  delta: '0:00:01.523232'
  end: '2019-07-04 02:29:19.337951'
  rc: 0
  start: '2019-07-04 02:29:17.814719'
  stderr: |-
    Generating a RSA private key
    ........................................................................++++
    .............................................++++
    writing new private key to 'ca.key'
    -----
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>

TASK [root-ca-certs : Converting Root CA key into pkcs1 format] ****************
changed: [127.0.0.1] => changed=true 
  cmd: openssl rsa -in /var/ansible/cfc-certs/root-ca/ca.key -out /var/ansible/cfc-certs/root-ca/ca.key.p1
  delta: '0:00:00.414985'
  end: '2019-07-04 02:29:20.077480'
  rc: 0
  start: '2019-07-04 02:29:19.662495'
  stderr: writing RSA key
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>

TASK [root-ca-certs-secrets : Create Root CA secret] ***************************
changed: [127.0.0.1] => changed=true 
  cmd: |-
    if [[ "klusterlet" == "icp" ]] || ! kubectl -n kube-system get secret cluster-ca-cert; then
     kubectl -n kube-system create secret tls cluster-ca-cert --cert=root-ca/ca.crt --key=root-ca/ca.key.p1 --dry-run -o yaml | kubectl apply -f -
     fi
  delta: '0:00:00.619971'
  end: '2019-07-04 02:29:21.012998'
  rc: 0
  start: '2019-07-04 02:29:20.393027'
  stderr: ''
  stderr_lines: <omitted>
  stdout: |-
    NAME              TYPE                DATA   AGE
    cluster-ca-cert   kubernetes.io/tls   2      32m
  stdout_lines: <omitted>

TASK [root-ca-certs-secrets : Creating ibmcloud-cluster-ca-cert rbac] **********
changed: [127.0.0.1] => changed=true 
  cmd: |-
    cat <<EOF | kubectl apply -f -
    ---
    kind: Role
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
      name: ibmcloud-cluster-ca-cert
      namespace: kube-public
    rules:
      - apiGroups: [""]
        resources: ["secret"]
        resourceNames: ["ibmcloud-cluster-ca-cert"]
        verbs: ["get"]
  
    ---
    kind: RoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
      name: ibmcloud-cluster-ca-cert
      namespace: kube-public
    subjects:
      - kind: Group
        apiGroup: rbac.authorization.k8s.io
        name: "system:authenticated"
      - kind: Group
        apiGroup: rbac.authorization.k8s.io
        name: "system:unauthenticated"
    roleRef:
      kind: Role
      name: ibmcloud-cluster-ca-cert
      apiGroup: rbac.authorization.k8s.io
    EOF
  delta: '0:00:00.726509'
  end: '2019-07-04 02:29:22.047194'
  rc: 0
  start: '2019-07-04 02:29:21.320685'
  stderr: ''
  stderr_lines: <omitted>
  stdout: |-
    role.rbac.authorization.k8s.io/ibmcloud-cluster-ca-cert unchanged
    rolebinding.rbac.authorization.k8s.io/ibmcloud-cluster-ca-cert unchanged
  stdout_lines: <omitted>

TASK [root-ca-certs-secrets : Creating ibmcloud-cluster-ca-cert secret] ********
changed: [127.0.0.1] => changed=true 
  cmd: |-
    cat <<EOF | kubectl apply -f -
    kind: Secret
    apiVersion: v1
    metadata:
      name: ibmcloud-cluster-ca-cert
      namespace: kube-public
    data:
      ca.crt: $(kubectl -n kube-system get secret cluster-ca-cert -o jsonpath='{.data.tls\.crt}')
    EOF
  delta: '0:00:00.732246'
  end: '2019-07-04 02:29:23.083231'
  rc: 0
  start: '2019-07-04 02:29:22.350985'
  stderr: ''
  stderr_lines: <omitted>
  stdout: secret/ibmcloud-cluster-ca-cert unchanged
  stdout_lines: <omitted>

TASK [k8s-helm-certs : Generating tiller and helm certificates] ****************
changed: [127.0.0.1] => changed=true 
  cmd: |-
    if ! kubectl -n kube-system get secret helm-tiller-secret; then
     kubectl -n kube-system get secret cluster-ca-cert -o jsonpath='{.data.tls\.crt}' | base64 -d > ca.crt
     kubectl -n kube-system get secret cluster-ca-cert -o jsonpath='{.data.tls\.key}' | base64 -d > ca.key
     export CERT_DIR=/tmp/helm
     export ROOT_CA_CRT=/tmp/ca.crt
     export ROOT_CA_KEY=/tmp/ca.key
     export ADMIN_USER=mjchang@tw.ibm.com
     /installer/playbook/roles/k8s-helm-certs/files/make-helm-certs.sh DNS:tiller-deploy,DNS:tiller-deploy.kube-system,DNS:tiller-deploy.kube-system:44134,DNS:tiller-deploy.kube-system.svc,DNS:localhost
  
     kubectl -n kube-system create secret generic helm-tiller-secret --from-file=key=helm/tiller.key --from-file=crt=helm/tiller.crt
     fi
  
     mkdir -p /var/ansible/cfc-certs/root-ca /var/ansible/cfc-certs/helm
     kubectl -n kube-system get secret cluster-ca-cert -o jsonpath='{.data.tls\.crt}' | base64 -d > /var/ansible/cfc-certs/root-ca/ca.crt
     kubectl -n kube-system get secret helm-tiller-secret -o jsonpath='{.data.crt}' | base64 -d > /var/ansible/cfc-certs/helm/tiller.crt
     kubectl -n kube-system get secret helm-tiller-secret -o jsonpath='{.data.key}' | base64 -d > /var/ansible/cfc-certs/helm/tiller.key
  delta: '0:00:02.112217'
  end: '2019-07-04 02:29:25.546517'
  rc: 0
  start: '2019-07-04 02:29:23.434300'
  stderr: ''
  stderr_lines: <omitted>
  stdout: |-
    NAME                 TYPE     DATA   AGE
    helm-tiller-secret   Opaque   2      32m
  stdout_lines: <omitted>

TASK [image-pull-secret : Waiting for default service account to exists] *******
skipping: [127.0.0.1] => (item=kube-system)  => changed=false 
  item: kube-system
  skip_reason: Conditional result was False
skipping: [127.0.0.1] => (item=cert-manager)  => changed=false 
  item: cert-manager
  skip_reason: Conditional result was False

TASK [image-pull-secret : Adding ImagePullSecrets to system namespace service account] ***
skipping: [127.0.0.1] => (item=kube-system)  => changed=false 
  item: kube-system
  skip_reason: Conditional result was False
skipping: [127.0.0.1] => (item=cert-manager)  => changed=false 
  item: cert-manager
  skip_reason: Conditional result was False

TASK [image-pull-secret : Waiting for default service account to exists] *******
skipping: [127.0.0.1] => (item=multicluster-endpoint)  => changed=false 
  item: multicluster-endpoint
  skip_reason: Conditional result was False

TASK [image-pull-secret : Adding ImagePullSecrets to system namespace service account] ***
skipping: [127.0.0.1] => (item=multicluster-endpoint)  => changed=false 
  item: multicluster-endpoint
  skip_reason: Conditional result was False

TASK [tiller : Ensuring component directory exist] *****************************
changed: [127.0.0.1] => changed=true 
  gid: 0
  group: root
  mode: '0700'
  owner: root
  path: /var/ansible/cfc-components
  size: 4096
  state: directory
  uid: 0

TASK [tiller : Ensuring that the tiller.yaml file exist] ***********************
changed: [127.0.0.1] => changed=true 
  checksum: e0be2d237e0e4f7c750ed29eaf1140c8f2f07031
  dest: /var/ansible/cfc-components/tiller.yaml
  gid: 0
  group: root
  md5sum: 6ca481d353a5126054bffe5c1446343f
  mode: '0600'
  owner: root
  size: 17189
  src: /tmp/ansible-tmp-1562207366.33-58683846953617/source
  state: file
  uid: 0

TASK [tiller : Deploying Tiller] ***********************************************
changed: [127.0.0.1] => changed=true 
  cmd: kubectl apply --force --overwrite=true -f /var/ansible/cfc-components/tiller.yaml
  delta: '0:00:00.763449'
  end: '2019-07-04 02:29:28.322037'
  rc: 0
  start: '2019-07-04 02:29:27.558588'
  stderr: ''
  stderr_lines: <omitted>
  stdout: |-
    secret/tiller-secret unchanged
    service/tiller-deploy unchanged
    deployment.extensions/tiller-deploy configured
  stdout_lines: <omitted>

TASK [tiller : Waiting for Tiller to start] ************************************
changed: [127.0.0.1] => changed=true 
  cmd: |-
    index=0
     retries=60
     while true; do
     sleep 10
     index=$(( index + 1 ))
     if [[ $index -eq $retries ]]; then
     echo "Failed to start Tiller, following is related info"
     kubectl -n kube-system get pods
     kubectl -n kube-system describe pod $(kubectl -n kube-system get pods -l app=helm,name=tiller -o jsonpath="{.items[0].metadata.name}")
     exit 1
     fi
     kubectl -n kube-system get pods --no-headers -l app=helm,name=tiller | awk '/Running/' | grep 1/1 && break
     done
  delta: '0:00:10.806337'
  end: '2019-07-04 02:29:39.876324'
  rc: 0
  start: '2019-07-04 02:29:29.069987'
  stderr: ''
  stderr_lines: <omitted>
  stdout: tiller-deploy-85f596fc5b-wsgzh   1/1   Running   0     11s
  stdout_lines: <omitted>

TASK [helm-config : Setting up Helm cli] ***************************************
changed: [127.0.0.1] => changed=true 
  attempts: 1
  cmd: |-
    helm init --client-only --skip-refresh
     export HELM_HOME=~/.helm
     kubectl -n kube-system get secret cluster-ca-cert -o jsonpath='{.data.tls\.crt}' | base64 -d > ca.crt
     kubectl -n kube-system get secret cluster-ca-cert -o jsonpath='{.data.tls\.key}' | base64 -d > ca.key
     openssl genrsa -out $HELM_HOME/key.pem 4096
     openssl req -new -key $HELM_HOME/key.pem -out $HELM_HOME/csr.pem -subj "/C=US/ST=New York/L=Armonk/O=IBM Cloud Private/CN=mjchang@tw.ibm.com"
     openssl x509 -req -in $HELM_HOME/csr.pem -extensions v3_usr -CA ca.crt -CAkey ca.key -CAcreateserial -out $HELM_HOME/cert.pem
     kubectl -n kube-system get pods -l app=helm,name=tiller
     index=0
     retries=10
     while true; do
     helm list --tls && break
     helm list --tls 1>/dev/null 2>/tmp/helm-error
     msg=$(< /tmp/helm-error)
     if [[ "$msg" =~ "incompatible versions" ]]; then
     cp /usr/local/helm /usr/local/bin/helm
     fi
     [[ $index -eq $retries ]] && break
     sleep 10
     index=$(( index + 1 ))
     done
  delta: '0:00:04.918344'
  end: '2019-07-04 02:29:45.127291'
  rc: 0
  start: '2019-07-04 02:29:40.208947'
  stderr: |-
    Generating RSA private key, 4096 bit long modulus (2 primes)
    ...........................................................................++++
    .............................................................................................................................................................................................................................++++
    e is 65537 (0x010001)
    Signature ok
    subject=C = US, ST = New York, L = Armonk, O = IBM Cloud Private, CN = mjchang@tw.ibm.com
    Getting CA Private Key
  stderr_lines: <omitted>
  stdout: |-
    Creating /root/.helm
    Creating /root/.helm/repository
    Creating /root/.helm/repository/cache
    Creating /root/.helm/repository/local
    Creating /root/.helm/plugins
    Creating /root/.helm/starters
    Creating /root/.helm/cache/archive
    Creating /root/.helm/repository/repositories.yaml
    Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com
    Adding local repo with URL: http://127.0.0.1:8879/charts
    $HELM_HOME has been configured at /root/.helm.
    Not installing Tiller due to 'client-only' flag having been set
    Happy Helming!
    NAME                             READY   STATUS    RESTARTS   AGE
    tiller-deploy-85f596fc5b-wsgzh   1/1     Running   0          16s
    NAME                    REVISION        UPDATED                         STATUS          CHART                   APP VERSION     NAMESPACE
    cert-manager            1               Thu Jul  4 01:57:17 2019        DEPLOYED        ibm-cert-manager-3.2.0  0.7.0           cert-manager
    monitoring              1               Thu Jul  4 01:57:25 2019        DEPLOYED        ibm-icpmonitoring-1.5.0                 kube-system
    multicluster-endpoint   1               Thu Jul  4 01:57:34 2019        DEPLOYED        ibm-klusterlet-3.2.0    1.0             multicluster-endpoint
  stdout_lines: <omitted>

TASK [chart-repo : Add Helm Chart Repository] **********************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : include_tasks] ***************************************************
included: /installer/playbook/roles/addon/tasks/release.yaml for 127.0.0.1

TASK [addon : Ensuring cert-manager resource directory exist] ******************
changed: [127.0.0.1] => changed=true 
  gid: 0
  group: root
  mode: '0700'
  owner: root
  path: /var/ansible/.addon/cert-manager
  size: 4096
  state: directory
  uid: 0

TASK [addon : Getting resource file for cert-manager chart] ********************
ok: [127.0.0.1] => changed=false 
  stat:
    exists: false

TASK [addon : Generating resource file for cert-manager chart] *****************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : Creating resources for cert-manager chart] ***********************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : Checking pre-install task file] **********************************
ok: [127.0.0.1] => changed=false 
  stat:
    exists: false

TASK [addon : include_tasks] ***************************************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : Generating values.yaml for cert-manager chart] *******************
changed: [127.0.0.1] => changed=true 
  checksum: e2e213e06a713d9f17adb9b2d491539542e9e514
  dest: /var/ansible/.addon/cert-manager/values.yaml
  gid: 0
  group: root
  md5sum: 13b5a116ccb1cb28e239f861fd612540
  mode: '0644'
  owner: root
  size: 473
  src: /tmp/ansible-tmp-1562207386.75-95295057627900/source
  state: file
  uid: 0

TASK [addon : Installing cert-manager chart] ***********************************
changed: [127.0.0.1] => changed=true 
  attempts: 1
  cmd: |-
    filename="/addon/ibm-cert-manager-3.2.0.tgz"
     if [[ -d "/addon/ibm-cert-manager-3.2.0.tgz" ]]; then
     filename=$(ls /addon/ibm-cert-manager-3.2.0.tgz/*.tgz | tail -1)
     fi
     ret=0
     if helm status --tls cert-manager &>/dev/null && helm status --tls cert-manager | grep -q 'STATUS: FAILED'; then
     rev=$(helm list --tls cert-manager | awk '{if($1 == "cert-manager"){print $2;exit}}')
     if [[ "$rev" == "1" ]]; then
     helm delete --tls --purge --timeout=600 cert-manager
     ret=$?
     sleep 5
     fi
     fi
     if ! helm status --tls cert-manager &>/dev/null; then
     helm install --tls --timeout=600  --name=cert-manager --namespace=cert-manager -f .addon/cert-manager/values.yaml $filename
     ret=$?
     elif false && helm status --tls cert-manager &>/dev/null; then
     helm upgrade --tls --timeout=600  cert-manager --namespace=cert-manager -f .addon/cert-manager/values.yaml $filename
     ret=$?
     fi
     if [[ $ret -ne 0 ]]; then
     tiller_pod=$(kubectl -n kube-system get pods -l app=helm,name=tiller -o jsonpath="{.items[0].metadata.name}")
     sleep 5
     kubectl -n kube-system logs $tiller_pod |& tail -n 500
     fi
     exit $ret
  delta: '0:00:03.277909'
  end: '2019-07-04 02:29:50.706478'
  rc: 0
  start: '2019-07-04 02:29:47.428569'
  stderr: ''
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>

TASK [addon : Checking post-install task file] *********************************
ok: [127.0.0.1] => changed=false 
  stat:
    atime: 1558620366.0
    attr_flags: ''
    attributes: []
    block_size: 4096
    blocks: 8
    charset: unknown
    checksum: 6429742cde22a4b9b379ff745da8cbf6b51777ed
    ctime: 1562205380.8028295
    dev: 51714
    device_type: 0
    executable: false
    exists: true
    gid: 0
    gr_name: root
    inode: 1797073
    isblk: false
    ischr: false
    isdir: false
    isfifo: false
    isgid: false
    islnk: false
    isreg: true
    issock: false
    isuid: false
    mimetype: unknown
    mode: '0664'
    mtime: 1558620366.0
    nlink: 1
    path: /installer/playbook/roles/addon/tasks/post-tasks/cert-manager.yaml
    pw_name: root
    readable: true
    rgrp: true
    roth: true
    rusr: true
    size: 2710
    uid: 0
    version: null
    wgrp: true
    woth: false
    writeable: true
    wusr: true
    xgrp: false
    xoth: false
    xusr: false

TASK [addon : include_tasks] ***************************************************
included: /installer/playbook/roles/addon/tasks/post-tasks/cert-manager.yaml for 127.0.0.1

TASK [addon : Waiting for ClusterIssuer CRD] ***********************************
changed: [127.0.0.1] => changed=true 
  attempts: 1
  cmd: |-
    kubectl api-resources | grep -q clusterissuers
    if [[ "$?" != 0 ]] ; then
      echo "Custom Resource Definition ClusterIssuers Not Ready."
    else
      echo "DONE"
    fi
  delta: '0:00:00.620031'
  end: '2019-07-04 02:29:52.089605'
  rc: 0
  start: '2019-07-04 02:29:51.469574'
  stderr: ''
  stderr_lines: <omitted>
  stdout: DONE
  stdout_lines: <omitted>

TASK [addon : Configuring cert-manager with Root CA Issuer] ********************
changed: [127.0.0.1] => changed=true 
  attempts: 1
  cmd: |-
    cat <<EOF | kubectl apply -f -
    apiVersion: certmanager.k8s.io/v1alpha1
    kind: ClusterIssuer
    metadata:
      name: icp-ca-issuer
    spec:
      ca:
        secretName: cluster-ca-cert
    EOF
    if [[ "$?" == "0" ]] ; then
      echo
      echo "----------------------- Cluster Issuer icp-ca-issuer ------------------------"
      findFalse=$(kubectl get clusterissuer icp-ca-issuer -o yaml --no-headers | grep False | tr -d \" | awk '{print $2}')
      status=$(echo $findFalse | awk '$1!="False" {print "READY"}')
      if [[ $status != "READY" ]] ; then
        echo "Cluster Issuer icp-ca-issuer is not ready."
        reason=$(kubectl get clusterissuer icp-ca-issuer -o yaml --no-headers | grep reason | awk '{print $2}')
        echo "Reason: $reason"
        cmPod=$(kubectl get pods -n cert-manager -o custom-columns=:metadata.name --no-headers | grep cert)
        if [[ -z $cmPod ]] ; then
          echo "Cert-manager pod doesn't exist."
        else
          cmStatus=$(kubectl get pods $cmPod -n cert-manager -o custom-columns=:status.phase --no-headers)
          if [[ "$cmStatus" == "Running" ]] ; then
            echo "----------------------- Pod ${cmPod} Logs -------------------------------"
            kubectl logs $(kubectl get pods -n cert-manager -o custom-columns=:metadata.name | grep cert) -n cert-manager | grep clusterissuer
          elif [[ "$cmStatus" == "Pending" ]] ; then
            echo "Pod ${cmPod} is not running."
            echo "----------------------- Pod ${cmPod} Events ---------------------"
            kubectl -n cert-manager get event -owide --field-selector involvedObject.name=${cmPod}
          fi
        fi
      else
        echo "DONE"
      fi
    else
      echo "Command to create cluster issuer did not complete successfully."
    fi
  delta: '0:00:00.860740'
  end: '2019-07-04 02:29:53.283321'
  rc: 0
  start: '2019-07-04 02:29:52.422581'
  stderr: ''
  stderr_lines: <omitted>
  stdout: |-
    clusterissuer.certmanager.k8s.io/icp-ca-issuer unchanged
  
    ----------------------- Cluster Issuer icp-ca-issuer ------------------------
    DONE
  stdout_lines: <omitted>

TASK [addon : include_tasks] ***************************************************
skipping: [127.0.0.1] => (item={'key': u'cert-manager', 'value': {u'path': u'/addon/ibm-cert-manager-3.2.0.tgz', u'namespace': u'cert-manager'}})  => changed=false 
  item:
    key: cert-manager
    value:
      namespace: cert-manager
      path: /addon/ibm-cert-manager-3.2.0.tgz
  skip_reason: Conditional result was False

TASK [addon : include_tasks] ***************************************************
included: /installer/playbook/roles/addon/tasks/release.yaml for 127.0.0.1

TASK [addon : Ensuring monitoring resource directory exist] ********************
changed: [127.0.0.1] => changed=true 
  gid: 0
  group: root
  mode: '0700'
  owner: root
  path: /var/ansible/.addon/monitoring
  size: 4096
  state: directory
  uid: 0

TASK [addon : Getting resource file for monitoring chart] **********************
ok: [127.0.0.1] => changed=false 
  stat:
    exists: false

TASK [addon : Generating resource file for monitoring chart] *******************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : Creating resources for monitoring chart] *************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : Checking pre-install task file] **********************************
ok: [127.0.0.1] => changed=false 
  stat:
    exists: false

TASK [addon : include_tasks] ***************************************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : Generating values.yaml for monitoring chart] *********************
changed: [127.0.0.1] => changed=true 
  checksum: a93d26d4c7c89310ba6188a896fd6dbe41a011dd
  dest: /var/ansible/.addon/monitoring/values.yaml
  gid: 0
  group: root
  md5sum: 11b51efe2a69c67e0c065d33bf913053
  mode: '0644'
  owner: root
  size: 1317
  src: /tmp/ansible-tmp-1562207395.36-98255102340743/source
  state: file
  uid: 0

TASK [addon : Installing monitoring chart] *************************************
changed: [127.0.0.1] => changed=true 
  attempts: 1
  cmd: |-
    filename="/addon/ibm-icpmonitoring-1.5.0.tgz"
     if [[ -d "/addon/ibm-icpmonitoring-1.5.0.tgz" ]]; then
     filename=$(ls /addon/ibm-icpmonitoring-1.5.0.tgz/*.tgz | tail -1)
     fi
     ret=0
     if helm status --tls monitoring &>/dev/null && helm status --tls monitoring | grep -q 'STATUS: FAILED'; then
     rev=$(helm list --tls monitoring | awk '{if($1 == "monitoring"){print $2;exit}}')
     if [[ "$rev" == "1" ]]; then
     helm delete --tls --purge --timeout=600 monitoring
     ret=$?
     sleep 5
     fi
     fi
     if ! helm status --tls monitoring &>/dev/null; then
     helm install --tls --timeout=600  --name=monitoring --namespace=kube-system -f .addon/monitoring/values.yaml $filename
     ret=$?
     elif false && helm status --tls monitoring &>/dev/null; then
     helm upgrade --tls --timeout=600  monitoring --namespace=kube-system -f .addon/monitoring/values.yaml $filename
     ret=$?
     fi
     if [[ $ret -ne 0 ]]; then
     tiller_pod=$(kubectl -n kube-system get pods -l app=helm,name=tiller -o jsonpath="{.items[0].metadata.name}")
     sleep 5
     kubectl -n kube-system logs $tiller_pod |& tail -n 500
     fi
     exit $ret
  delta: '0:00:02.161484'
  end: '2019-07-04 02:29:58.366538'
  rc: 0
  start: '2019-07-04 02:29:56.205054'
  stderr: ''
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>

TASK [addon : Checking post-install task file] *********************************
ok: [127.0.0.1] => changed=false 
  stat:
    exists: false

TASK [addon : include_tasks] ***************************************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : include_tasks] ***************************************************
skipping: [127.0.0.1] => (item={'key': u'monitoring', 'value': {u'path': u'/addon/ibm-icpmonitoring-1.5.0.tgz', u'namespace': u'kube-system'}})  => changed=false 
  item:
    key: monitoring
    value:
      namespace: kube-system
      path: /addon/ibm-icpmonitoring-1.5.0.tgz
  skip_reason: Conditional result was False

TASK [addon : include_tasks] ***************************************************
included: /installer/playbook/roles/addon/tasks/release.yaml for 127.0.0.1

TASK [addon : Ensuring multicluster-endpoint resource directory exist] *********
changed: [127.0.0.1] => changed=true 
  gid: 0
  group: root
  mode: '0700'
  owner: root
  path: /var/ansible/.addon/multicluster-endpoint
  size: 4096
  state: directory
  uid: 0

TASK [addon : Getting resource file for multicluster-endpoint chart] ***********
ok: [127.0.0.1] => changed=false 
  stat:
    exists: false

TASK [addon : Generating resource file for multicluster-endpoint chart] ********
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : Creating resources for multicluster-endpoint chart] **************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : Checking pre-install task file] **********************************
ok: [127.0.0.1] => changed=false 
  stat:
    atime: 1558620366.0
    attr_flags: ''
    attributes: []
    block_size: 4096
    blocks: 16
    charset: unknown
    checksum: 8d98721d5cd36620ca3ba2f7ac0ee507b5d94d38
    ctime: 1562205380.8028295
    dev: 51714
    device_type: 0
    executable: false
    exists: true
    gid: 0
    gr_name: root
    inode: 1797076
    isblk: false
    ischr: false
    isdir: false
    isfifo: false
    isgid: false
    islnk: false
    isreg: true
    issock: false
    isuid: false
    mimetype: unknown
    mode: '0664'
    mtime: 1558620366.0
    nlink: 1
    path: /installer/playbook/roles/addon/tasks/pre-tasks/multicluster-endpoint.yaml
    pw_name: root
    readable: true
    rgrp: true
    roth: true
    rusr: true
    size: 5369
    uid: 0
    version: null
    wgrp: true
    woth: false
    writeable: true
    wusr: true
    xgrp: false
    xoth: false
    xusr: false

TASK [addon : include_tasks] ***************************************************
 [WARNING]: Ignoring invalid attribute: management_services
included: /installer/playbook/roles/addon/tasks/pre-tasks/multicluster-endpoint.yaml for 127.0.0.1

TASK [addon : Adding label to multicluster-endpoint namespaces] ****************
changed: [127.0.0.1] => changed=true 
  cmd: |-
    kubectl get namespace multicluster-endpoint || kubectl create namespace multicluster-endpoint
     kubectl label namespace multicluster-endpoint icp=system --overwrite=true
  delta: '0:00:00.728367'
  end: '2019-07-04 02:30:01.332666'
  rc: 0
  start: '2019-07-04 02:30:00.604299'
  stderr: ''
  stderr_lines: <omitted>
  stdout: |-
    NAME                    STATUS   AGE
    multicluster-endpoint   Active   32m
    namespace/multicluster-endpoint not labeled
  stdout_lines: <omitted>

TASK [addon : Set install_klusterlet_playbook default] *************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : Set deployed_on_hub default] *************************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : Set deployed_on_hub] *********************************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : Set multicluster_hub_api_token when deployed_on_hub] *************
skipping: [127.0.0.1] => changed=false 
  censored: 'the output has been hidden due to the fact that ''no_log: true'' was specified for this result'

TASK [addon : Generate klusterlet-bootstrap kubeconfig when deployed_on_hub] ***
skipping: [127.0.0.1] => changed=false 
  censored: 'the output has been hidden due to the fact that ''no_log: true'' was specified for this result'

TASK [addon : Create klusterlet-bootstrap secret when deployed_on_hub] *********
skipping: [127.0.0.1] => changed=false 
  censored: 'the output has been hidden due to the fact that ''no_log: true'' was specified for this result'

TASK [addon : Check that the klusterlet-bootstrap.kubeconfig exists when not deployed_on_hub] ***
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : Generate klusterlet-bootstrap kubeconfig when not deployed_on_hub] ***
skipping: [127.0.0.1] => changed=false 
  censored: 'the output has been hidden due to the fact that ''no_log: true'' was specified for this result'

TASK [addon : Create klusterlet-bootstrap secret when not deployed_on_hub] *****
skipping: [127.0.0.1] => changed=false 
  censored: 'the output has been hidden due to the fact that ''no_log: true'' was specified for this result'

TASK [addon : Set variables for ICP] *******************************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : Set variables for ICP on OpenShift] ******************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : Set variables for ICP on IKS] ************************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : Override clusterName to local-cluster when deployed_on_hub] ******
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : Override clusterNamespace to local-cluster when deployed_on_hub] ***
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : Generating values.yaml for multicluster-endpoint chart] **********
changed: [127.0.0.1] => changed=true 
  checksum: ebad62908af210b50a9760e66e22f7d750da5d45
  dest: /var/ansible/.addon/multicluster-endpoint/values.yaml
  gid: 0
  group: root
  md5sum: 9a70dd056674b95b750c33309eb6381e
  mode: '0644'
  owner: root
  size: 1126
  src: /tmp/ansible-tmp-1562207402.06-71520898182076/source
  state: file
  uid: 0

TASK [addon : Installing multicluster-endpoint chart] **************************
changed: [127.0.0.1] => changed=true 
  attempts: 1
  cmd: |-
    filename="/addon/ibm-klusterlet-3.2.0.tgz"
     if [[ -d "/addon/ibm-klusterlet-3.2.0.tgz" ]]; then
     filename=$(ls /addon/ibm-klusterlet-3.2.0.tgz/*.tgz | tail -1)
     fi
     ret=0
     if helm status --tls multicluster-endpoint &>/dev/null && helm status --tls multicluster-endpoint | grep -q 'STATUS: FAILED'; then
     rev=$(helm list --tls multicluster-endpoint | awk '{if($1 == "multicluster-endpoint"){print $2;exit}}')
     if [[ "$rev" == "1" ]]; then
     helm delete --tls --purge --timeout=600 multicluster-endpoint
     ret=$?
     sleep 5
     fi
     fi
     if ! helm status --tls multicluster-endpoint &>/dev/null; then
     helm install --tls --timeout=600  --name=multicluster-endpoint --namespace=multicluster-endpoint -f .addon/multicluster-endpoint/values.yaml $filename
     ret=$?
     elif false && helm status --tls multicluster-endpoint &>/dev/null; then
     helm upgrade --tls --timeout=600  multicluster-endpoint --namespace=multicluster-endpoint -f .addon/multicluster-endpoint/values.yaml $filename
     ret=$?
     fi
     if [[ $ret -ne 0 ]]; then
     tiller_pod=$(kubectl -n kube-system get pods -l app=helm,name=tiller -o jsonpath="{.items[0].metadata.name}")
     sleep 5
     kubectl -n kube-system logs $tiller_pod |& tail -n 500
     fi
     exit $ret
  delta: '0:00:03.007887'
  end: '2019-07-04 02:30:05.750159'
  rc: 0
  start: '2019-07-04 02:30:02.742272'
  stderr: ''
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>

TASK [addon : Checking post-install task file] *********************************
ok: [127.0.0.1] => changed=false 
  stat:
    exists: false

TASK [addon : include_tasks] ***************************************************
skipping: [127.0.0.1] => changed=false 
  skip_reason: Conditional result was False

TASK [addon : include_tasks] ***************************************************
skipping: [127.0.0.1] => (item={'key': u'multicluster-endpoint', 'value': {u'path': u'/addon/ibm-klusterlet-3.2.0.tgz', u'namespace': u'multicluster-endpoint', u'use_custom_template': True}})  => changed=false 
  item:
    key: multicluster-endpoint
    value:
      namespace: multicluster-endpoint
      path: /addon/ibm-klusterlet-3.2.0.tgz
      use_custom_template: true
  skip_reason: Conditional result was False

PLAY RECAP *********************************************************************
127.0.0.1                  : ok=51   changed=30   unreachable=0    failed=0   

Playbook run took 0 days, 0 hours, 1 minutes, 4 seconds

waiting for the cluster join request to be created
